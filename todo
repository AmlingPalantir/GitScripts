G3MD
    resolvers
        [something to] handle indent conflict better by semimatching projected lines so we only conflict on the relevant part
            in particular we allow ("a","a"," a") -> " a" but not e.g.  ("  a","a"," a")
        maybe bring slow/exact character merge back
            that example from Carl was handled way less well by `git merge-file`
    "auto" resolvers tried automatically on each block (right now "g" is sort of being used like this but we can make it first class and have an option to add "mq" as well)
        conceivably one might want to force "mr" delegation as well but that's less clear...
    redo overall recursive structure
        in particular we'd like to be able to view context and in some cases merge with context
        this will require special work for punt and m*
        in particular, m* structure will almost certainly have to change entirely
GRD
    various cleanup (in script as TODO)
    maybe try to figure out "already merged" changes and skip them
        git-cherry/git-patch-id can give us the data we need
        unclear in general case what with the multiple places we could be playing onto...
            maybe invoking cherry on each onto/plus pair and then seeding the ontos as containing those commits?
            we'd also need to have pick generate as NOP if it's already "contained" in the top
    allow return to editting script at some failure points
        ideally we'd dump the saved map, stack, etc.  into the script and so just drop state and run the new script from the beginning
GBD
    track good/bad sets and be able to view the history (similar to "git bisect log")
    probabilistic bisect
        this is where you can run but a "good" doesn't mean good, it means it didn't show
        there's no end condition, just the current best guess and the confidence for it
        unclear if this should be part of GBD or be its own tool
    split good and presumed good
        something tested is "good", ancestors not yet tested are "presumed good"
        only difference is if we get to bad with "presumed good" parents we have to test them
            if one of them tests bad we work from there again
            if all of them test good we're done
    parallel automated bisect
        again, unclear if this is part of GBD or own tool
        another possibility here is your run script produces a collection of failures
        by end of run it needs to be the case that every known (commit, failure) pair has an originating ancestor
            originating ancestor is an ancestor that has the failure all of whose ancestors do not
        err, maybe the weaker requirement that every observed failure has an originating point
            this is more similar to what bisect does right now
        or even weaker, that every observed failure in the tip commit has an originating ancestor
            then we just give:
                a tip commit
                a script to produce failure list
                an "assume good" ancestor (to avoid checking all of history)
                    or maybe we insist your script should check being an ancestor?
                        uh, in parallel bisect you don't even git a repo, you just get a WT
    maybe GBD needs to be split into state library plus
        vanilla GBD frontend (manual, single WT, as current GBD)
        probabilistic bisect
        parallel external WT bisect
        vanilla and parallel are both addressed by set-good, set-bad, and find-test-point (parallel speculates)
        probabilistic needs something else though
            we sort of want to track an earliest known bad and what goods have happened earlier than that
            we can probably get away with running brute-force MLE for bad commit
                actually, no
                this is exactly doing a regular bisect and then hammering the parents (and if they break repeating from there)
                this does horribly in the case of linear history and a low probability failure
            unclear how weight is relevant here
                maybe maximize (likelihood * weight) instead...
                this is sort of weird because weight is forced to interact with likelihood multiplicatively
                it means you're going to test the shit out of high weights and wait a long, long time before anyone else becomes likely enough to counter
                I think weight just may be a bad idea here
            yeah, I think this doesn't share any data structure code
            in addition probabilistic sort of wants to parallelize as well...
github pull requester
    worked: curl -n -d '{"title":"Please pull upstream/20110915-typo-fix","base":"8e90ecf3b8edae14b1b34663a7e0298040bf6435","head":"5a9814923942b2099862aa6503c8af8bd7b3e7d9"}' 'https://api.github.com/repos/benbernard/RecordStream/pulls'
    also worked: curl -n -d '{"title":"Please pull upstream/20110915-other-doc","base":"benbernard:master","head":"amling:upstream/20110915-other-doc"}' 'https://api.github.com/repos/benbernard/RecordStream/pulls'
